{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MI_Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlhgY52CC47lO4Kn33SFx0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siribafna/Myocardial-Infarction-Predictions/blob/main/MI_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0ClhDt0Q-4n"
      },
      "outputs": [],
      "source": [
        "#%% Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jECe9BV_TzVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get data from csv\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/CS 4475/Myocardial infarction complications Database.csv\")"
      ],
      "metadata": {
        "id": "is76OyeXT1sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogisticR-Simple"
      ],
      "metadata": {
        "id": "AGujnoA_VdoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Preprocessing\n",
        "# Splitting our data set in dependent and independent variables\n",
        "X=dataset.iloc[:,[2,3]].values\n",
        "y=dataset.iloc[:,4].values\n",
        "# Spliting the data set into the Training Set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_stat= 0)\n",
        "\n",
        "#%% Additional Preprocessing\n",
        "# Sometimes it is convenient to transform the data. \n",
        "# A common approach consist in standardize the data such that \n",
        "# all variables are in the same 'range'\n",
        "# i.e. here next code is based on the normal standard density\n",
        "# X = mu+sigma*Z\n",
        "sc_X = StandardScalar()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "#%% Logistic Regression on the Training Set\n",
        "classifier = LogisticRegression(random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "#%% to predict the Teset set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "#%% Building the Confusion \n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "#%% Visualization of Training Set Results\n",
        "X_set, y_set = X_train, y_train\n",
        "X1, X2 = np. meshgrid(np.arange(start = X_set[:, 0].min()-1, stop = X_set[:, 0].max() +1, step = 0.01),\n",
        "                      np.arange(start = X_set[:, 1].min()-1, stop = X_set[:, 1].max()+1,step = 0.01))\n",
        "plt.contourf(X1, X2 classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "            alpha = 0.75, cmap = ListedColormap(('red','green')))\n",
        "plt.xllim(X1.min90, X1.max())\n",
        "plt.ylim(X2.min(),X2.max())\n",
        "for 1, j in enumerate (np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red','green'))(i), label = j)\n",
        "    plt.title('Logistic Regression (Training Set)')\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Estimated Salary')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#%% Visualization of Test Set Results\n",
        "X_set, y_set = X_test, y_test\n",
        "X1, X2 = np. meshgrid(np.arange(start = X_set[:, 0].min()-1, stop = X_set[:, 0].max() +1, step = 0.01),\n",
        "                      np.arange(start = X_set[:, 1].min()-1, stop = X_set[:, 1].max()+1,step = 0.01))\n",
        "plt.contourf(X1, X2 classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "            alpha = 0.75, cmap = ListedColormap(('red','green')))\n",
        "plt.xllim(X1.min90, X1.max())\n",
        "plt.ylim(X2.min(),X2.max())\n",
        "for 1, j in enumerate (np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red','green'))(i), label = j)\n",
        "    plt.title('Logistic Regression (Training Set)')\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Estimated Salary')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8Yjbm3-fVdNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogisticR-Exploratory"
      ],
      "metadata": {
        "id": "lfqt_mnpWZHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_histograms(dataframe, features, rows, cols):\n",
        "    fig=plt.figure(figsize=(20,20))\n",
        "    for i, feature in enumerate(features):\n",
        "        ax=fig.add_subplot(rows,cols,i+1)\n",
        "        dataframe[feature].hist(bins=20,ax=ax,facecolor=colors[i])\n",
        "        ax.set_title(feature+\" Histogram\",color=colors[35])\n",
        "        ax.set_yscale('log')\n",
        "    fig.tight_layout() \n",
        "    plt.savefig('Histograms.png')\n",
        "    plt.show()\n",
        "draw_histograms(df,df.columns,8,4)\n",
        "\n",
        "\n",
        "#%% EXPLORATORY DATA VISUALIZATION (EXTRA)\n",
        "# Correlation between each pair of variables\n",
        "plt.figure(figsize = (38,16))\n",
        "sns.heatmap(df.corr(), annot = True)\n",
        "#plt.savefig('heatmap.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#%% Applying Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "#%% Evaluating our model (prediction stage)\n",
        "train_acc = model.score(X_train_scaled, y_train)\n",
        "print(\"The Accuracy for Training Set is {}\".format(train_acc*100))\n",
        "# Over 99.9% accuracy, which is pretty good, but training accuracy \n",
        "# is not that useful, test accuracy is the real metric of success.\n",
        "\n",
        "# TEST - Prediction\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"The Accuracy for Test Set is {}\".format(test_acc*100))\n",
        "# The test accuracy is also over 99.9% which is great.\n",
        "\n",
        "\n",
        "#%% To generate a classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "#%% Visualization using confusion matrix\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "sns.heatmap(cm, annot=True,fmt='d', cmap='Blues')\n",
        "plt.ylabel(\"Actual Values\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "#plt.savefig('confusion_matrix.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UM2_GRZ_VdBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}